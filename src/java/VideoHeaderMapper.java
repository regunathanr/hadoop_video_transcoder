import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.fs.*;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import java.util.Random;

public class VideoHeaderMapper extends Mapper<LongWritable,BytesWritable,Text, IntWritable>
{


   private native int mpeg2decHeaderMain(byte[] bitstream_buffer,int buf_size,int[] sequence_info_arr);


  int total_len = 0;
  int decode_called = 0;
  int curpos = 0;
  int temp_byte_array_filled = 0;
  

   static {
      System.err.println(" 123 regu-victor before loading");
      System.loadLibrary("mpeg2dec_jni");//This is firstJNI.DLL
      System.err.println("123 regu-victor");
        /*if generated by borland
         System.loadLibrary("firstjni");//This is firstjni.dll
         */
   }

      //hadoop supported data types
      private final static IntWritable one = new IntWritable(1);
      private Text word = new Text();
     
      //map method that performs the tokenizer job and framing the initial key value pairs
      public void map(LongWritable key, BytesWritable value,Context context) throws IOException,InterruptedException
      {

	// JNI load the var value 

        

        //sequence info
        int[] sequence_info = new int[18];
       /*sequence_info[0] = 720;
       sequence_info[1] = 576;
       sequence_info[2] = 360;
       sequence_info[3] = 576;
       sequence_info[4] = 13107150;
       sequence_info[5] = 356352;
       sequence_info[6] = 165;
       sequence_info[7] = 720;
       sequence_info[8] = 576;  
       sequence_info[9] = 720;
       sequence_info[10] = 576;
       sequence_info[11] = 1;
       sequence_info[12] = 1;
       sequence_info[13] = 1080000;
       sequence_info[14] = 133;
       sequence_info[15] = 0;
       sequence_info[16] = 0;
       sequence_info[17] = 0;*/

        int fileDatalen = value.getLength();
        total_len = total_len + fileDatalen;
        System.out.println("fileDatalen: "+fileDatalen);
       // System.err.println("first 4 bytes:"+fileData[0]+fileData[1]+fileData[2]+fileData[3]);

	//System.arraycopy(fileData,0,temp_byte_array,curpos,fileDatalen);
        curpos = curpos + fileDatalen;


         if(key.get() == 0)
         {
		//implies that it is the first chunk

             
          	mpeg2decHeaderMain(value.getBytes(),fileDatalen,sequence_info);
                decode_called = 1;
                System.out.println("chunkid: "+ key.get());
                
               int end_ind;
               int length_in_bytes;
               int start_ind = 0; 
               
               FileSystem fs =  FileSystem.get(context.getConfiguration());

              
               String filename_val = ((FileSplit)context.getInputSplit()).getPath().getName() +"_sequence_info.txt";
	       //String filename_val = context.getJobName()+"_sequence_info.txt";
               Path outFile = new Path(filename_val);
               FSDataOutputStream out = fs.create(outFile);
               
               for(int i = 0; i < 18; i++) 
               {
                  out.writeUTF(Integer.toString(sequence_info[i]));
                  out.writeUTF("\n");
                    
               }
               out.close();
          
        }
       else
       {
		System.err.println("not first chunk: "+key.get()); 
       }
      System.err.println("curpos : " + curpos);
   
	// feed value into JNI

            //taking one line at a time and tokenizing the same
          
          byte[] tempvalue = new byte[256];
          System.arraycopy(value.getBytes(),0,tempvalue,0,256);
          String line = tempvalue.toString();//temp_text.toString();
          StringTokenizer tokenizer = new StringTokenizer(line);
         
          //iterating through all the words available in that line and forming the key value pair
            while (tokenizer.hasMoreTokens())
            {
               word.set(tokenizer.nextToken());
               //sending to output collector which inturn passes the same to reducer
                // output.collect(word, one);
                context.write(word,one);
            }
	
       }
}


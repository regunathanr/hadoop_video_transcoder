import java.io.IOException;
import java.util.StringTokenizer;
import java.io.*;

import org.apache.hadoop.io.*;
//import org.apache.hadoop.mapred.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.fs.*;
import java.util.Random;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

//public class WordCountMapper extends MapReduceBase implements Mapper<NullWritable, BytesWritable, Text, IntWritable>
//public class WordCountMapper extends Mapper<LongWritable,BytesWritable,Text, IntWritable>
public class WordCountMapper extends Mapper<LongWritable,BytesWritable,Text, BytesWritable>
{


   private native int mpeg2decMain(byte[] bitstream_buffer,int buf_size,int[] sequence_info_arr,byte[] jpeg_output_buffer_java, int[] jpg_indexvec, int output_bufsize,int[] actual_num_frames_written_to_buf,int num_frames_to_skip);


  int total_len = 0;
  //static byte[] temp_byte_array = new byte[67108864];
 // int temp_byte_array_len = 67108864;
  int decode_called = 0;
  int curpos = 0;
  int temp_byte_array_filled = 0;
  

   static {
      System.err.println(" 123 regu-victor before loading");
      System.loadLibrary("mpeg2dec_jni");//This is firstJNI.DLL
      System.err.println("123 regu-victor");
        /*if generated by borland
         System.loadLibrary("firstjni");//This is firstjni.dll
         */
   }

      //hadoop supported data types
      private final static IntWritable one = new IntWritable(1);
      private Text word = new Text();
      private BytesWritable tempbyteswritable = new BytesWritable();
     
      //map method that performs the tokenizer job and framing the initial key value pairs
     // public void map(NullWritable key, BytesWritable value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException
      public void map(LongWritable key, BytesWritable value,Context context) throws IOException,InterruptedException
      {

	// JNI load the var value 

       // mpeg2decJNIcall JN=new mpeg2decJNIcall();
        
      // Log log = LogFactory.getLog(WordCountMapper.class);
        int num_frames_to_skip = 3;

        
       System.err.println(FileOutputFormat.getOutputPath(context));
         //sequence info
        int[] sequence_info = new int[18];

      String seq_filename;
      String hdfs_dir = "/user/serengeti/";
     
      seq_filename = hdfs_dir + context.getJobName()+"_sequence_info.txt";

      Path pt = new Path(seq_filename);
      FileSystem fs1= FileSystem.get(context.getConfiguration());
      BufferedReader br = new BufferedReader(new InputStreamReader(fs1.open(pt)));
      
      try
      { 
          String line1;
          for(int idx = 0; idx < 18; idx++)
	  { 
          	line1 = br.readLine();
                String line2 = line1.trim(); 
                sequence_info[idx] = Integer.valueOf(line2);
                System.err.println("seq info: "+ sequence_info[idx]);
          }
      }
      finally
      {
	  br.close();
      }
          

       /*sequence_info[0] = 720;
       sequence_info[1] = 576;
       sequence_info[2] = 360;
       sequence_info[3] = 576;
       sequence_info[4] = 13107150;
       sequence_info[5] = 356352;
       sequence_info[6] = 165;
       sequence_info[7] = 720;
       sequence_info[8] = 576;  
       sequence_info[9] = 720;
       sequence_info[10] = 576;
       sequence_info[11] = 1;
       sequence_info[12] = 1;
       sequence_info[13] = 1080000;
       sequence_info[14] = 133;
       sequence_info[15] = 0;
       sequence_info[16] = 0;
       sequence_info[17] = 0;*/

        int fileDatalen = value.getLength();
        //byte[] fileData = new byte[fileDatalen];
        //fileData = value.getBytes();
        
        total_len = total_len + fileDatalen;
        System.out.println("fileDatalen: "+fileDatalen);
       // System.err.println("first 4 bytes:"+fileData[0]+fileData[1]+fileData[2]+fileData[3]);

	//System.arraycopy(fileData,0,temp_byte_array,curpos,fileDatalen);
        curpos = curpos + fileDatalen;

      //int size_est_one_jpeg = 320000;//320kbytes is estimated size of one jpeg file
       // int est_num_jpg_frames = 5;
       // byte[] jpg_outputbuffer_java = new byte[size_est_one_jpeg*est_num_jpg_frames];
       // int[] jpg_indexvec = new int[est_num_jpg_frames*2];
        int[] actual_num_frames_written_to_buf=new int[1];


       //System.err.println(fileDatalen+est_num_jpg_frames);
       //System.err.println("total len: "+total_len);
       //if(total_len > temp_byte_array_len)
      // if(temp_byte_array_filled == 1)
       //{
          
          //if(true)//(decode_called == 0)
         // {
                int size_est_one_jpeg = 320000;//320kbytes is estimated size of one jpeg file
               int est_num_jpg_frames = 808;
                byte[] jpg_outputbuffer_java = new byte[size_est_one_jpeg*est_num_jpg_frames];
                int[] jpg_indexvec = new int[est_num_jpg_frames*2];

                Random randgen = new Random();
                int randint = randgen.nextInt(100);
             
          	mpeg2decMain(value.getBytes(),fileDatalen,sequence_info,jpg_outputbuffer_java,jpg_indexvec,size_est_one_jpeg*est_num_jpg_frames,actual_num_frames_written_to_buf,num_frames_to_skip);
                decode_called = 1;
                String msg = "actual num frames decoded: "+ actual_num_frames_written_to_buf[0];
                System.out.println(msg);
                System.out.println("chunkid: "+ randint);
                
               int end_ind;
               int length_in_bytes;
               int start_ind = 0; 
               
               FileSystem fs =  FileSystem.get(context.getConfiguration());

                //write to jpg files in hdfs
              for(int i = 0;i < actual_num_frames_written_to_buf[0];i++)
               
               //start_ind = jpg_indexvec[actual_num_frames_written_to_buf[0]-12-1];
               //for(int i = actual_num_frames_written_to_buf[0]-12; i < actual_num_frames_written_to_buf[0]-1; i++) 
               {
          	   end_ind = jpg_indexvec[i]-1;
                   length_in_bytes = end_ind - start_ind+1;
                   byte [] temp_buf = new byte[8+end_ind-start_ind+1];
                   //System.arraycopy(jpg_outputbuffer_java,start_ind,temp_buf,0,length_in_bytes);
                   start_ind = end_ind+1;
                   
                   long chunk_val = key.get()/(64*1024*1024);


                   int actual_framenum;
                   if(num_frames_to_skip != 0)
		   {
                      actual_framenum = i*num_frames_to_skip;
                   }
		   else
		   {
			actual_framenum = i;
		   }
                   
                   ByteArrayOutputStream bos = new ByteArrayOutputStream();
                   DataOutputStream dos = new DataOutputStream(bos);
                   dos.writeLong((long)actual_framenum);
                   dos.flush();
                   dos.close();
                   byte[] frameid_bytes = bos.toByteArray();
                   
                   System.err.println("framnum"+actual_framenum +"b0"+ frameid_bytes[0]+frameid_bytes[1]+frameid_bytes[6]+frameid_bytes[7]);
		   System.arraycopy(frameid_bytes,0,temp_buf,0,8);
                   System.arraycopy(jpg_outputbuffer_java,start_ind,temp_buf,8,length_in_bytes);
  
                   String filename_val = context.getJobName()+"_"+chunk_val+"_"+actual_framenum+"_map.jpg"; 
                   String reducer_key = context.getJobName()+"_"+chunk_val;                  
                   //Path outFile = new Path("frame"+key.get()+"_"+i+".jpg");
                   //if(i > actual_num_frames_written_to_buf[0]-12)
		   if((i > 26)&&(i < 38))
		   {	
                   	Path outFile = new Path(filename_val);
                   	FSDataOutputStream out = fs.create(outFile);
                   	out.write(temp_buf,8,length_in_bytes);
                   	out.close();
		   }
                   //BytesWritable tempbyteswritable = new BytesWritable(tempbuf);
                   word.set(reducer_key);
                   tempbyteswritable.set(temp_buf,0,length_in_bytes+8);
                   context.write(word,tempbyteswritable);
                   //byte[] b1 = new byte[1];
                  // b1[0] = (byte)0xad;
                  // BytesWritable tempbytes1 = new BytesWritable(b1); 
                  // context.write(word,tempbytes1);  
               }
          
          //}
      //}
      //else
      //{ 
        //  if(curpos + fileDatalen < temp_byte_array_len)
         // {
           //    System.arraycopy(fileData,0,temp_byte_array,curpos,fileDatalen);
             //  curpos = curpos + fileDatalen;
               //temp_byte_array[curpos] =0x0A;//new line char; 
               //curpos = curpos+1;
           
         // }
         // else
         // {
           //   int rem_bytes = temp_byte_array_len-curpos;
             // System.arraycopy(fileData,0,temp_byte_array,curpos,rem_bytes);
             // curpos = curpos + rem_bytes;
             // temp_byte_array_filled = 1;
              //System.err.println("byte array filled");
         // }
     // }
      System.err.println("curpos : " + curpos);
   
      // String msg = "actual num frames decoded: "+ actual_num_frames_written_to_buf[0];
       //System.err.println(msg);       
//log.info(msg);
	// feed value into JNI

            //taking one line at a time and tokenizing the same
          //Text temp_text =  new Text(value.toString());
          
          byte[] tempvalue = new byte[256];
          System.arraycopy(value.getBytes(),0,tempvalue,0,256);
          String line = tempvalue.toString();//temp_text.toString();
          StringTokenizer tokenizer = new StringTokenizer(line);
         
          //iterating through all the words available in that line and forming the key value pair
           // while (tokenizer.hasMoreTokens())
           // {
              // word.set(tokenizer.nextToken());
               //sending to output collector which inturn passes the same to reducer
                // output.collect(word, one);
               // context.write(word,one);
           // }
	
       }
}


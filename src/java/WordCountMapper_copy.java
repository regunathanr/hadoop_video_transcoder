import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;
//import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.fs.*;
import java.util.Random;

public class WordCountMapper extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable>
//public class WordCountMapper extends Mapper<LongWritable,Text,Text, IntWritable>
{


   private native int mpeg2decMain(byte[] bitstream_buffer,int buf_size,int[] sequence_info_arr,byte[] jpeg_output_buffer_java, int[] jpg_indexvec, int output_bufsize,int[] actual_num_frames_written_to_buf);


  int total_len = 0;
  static byte[] temp_byte_array = new byte[56000000];
  int temp_byte_array_len = 56000000;
  int decode_called = 0;
  int curpos = 0;
  int temp_byte_array_filled = 0;
  

   static {
      System.err.println(" 123 regu-victor before loading");
      System.loadLibrary("mpeg2dec_jni");//This is firstJNI.DLL
      System.err.println("123 regu-victor");
        /*if generated by borland
         System.loadLibrary("firstjni");//This is firstjni.dll
         */
   }

      //hadoop supported data types
      private final static IntWritable one = new IntWritable(1);
      private Text word = new Text();
     
      //map method that performs the tokenizer job and framing the initial key value pairs
      public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException
     // public map(LongWritable key, Text value,Context context) throws IOException,InterruptedException
      {

	// JNI load the var value 

       // mpeg2decJNIcall JN=new mpeg2decJNIcall();
        
      // Log log = LogFactory.getLog(WordCountMapper.class);

        //sequence info
        int[] sequence_info = new int[18];
       sequence_info[0] = 720;
       sequence_info[1] = 576;
       sequence_info[2] = 360;
       sequence_info[3] = 576;
       sequence_info[4] = 13107150;
       sequence_info[5] = 356352;
       sequence_info[6] = 165;
       sequence_info[7] = 720;
       sequence_info[8] = 576;  
       sequence_info[9] = 720;
       sequence_info[10] = 576;
       sequence_info[11] = 1;
       sequence_info[12] = 1;
       sequence_info[13] = 1080000;
       sequence_info[14] = 133;
       sequence_info[15] = 0;
       sequence_info[16] = 0;
       sequence_info[17] = 0;

        int fileDatalen = value.getLength();
        byte[] fileData = new byte[fileDatalen];
        fileData = value.getBytes();
        total_len = total_len + fileDatalen;

      //int size_est_one_jpeg = 320000;//320kbytes is estimated size of one jpeg file
       // int est_num_jpg_frames = 5;
       // byte[] jpg_outputbuffer_java = new byte[size_est_one_jpeg*est_num_jpg_frames];
       // int[] jpg_indexvec = new int[est_num_jpg_frames*2];
        int[] actual_num_frames_written_to_buf=new int[1];


       //System.err.println(fileDatalen+est_num_jpg_frames);
       //System.err.println("total len: "+total_len);
       //if(total_len > temp_byte_array_len)
       if(temp_byte_array_filled == 1)
       {
          
          if(decode_called == 0)
          {
                int size_est_one_jpeg = 320000;//320kbytes is estimated size of one jpeg file
                int est_num_jpg_frames = 100;
                byte[] jpg_outputbuffer_java = new byte[size_est_one_jpeg*est_num_jpg_frames];
                int[] jpg_indexvec = new int[est_num_jpg_frames*2];

                Random randgen = new Random();
                int randint = randgen.nextInt(100);
             
          	mpeg2decMain(temp_byte_array,temp_byte_array_len,sequence_info,jpg_outputbuffer_java,jpg_indexvec,size_est_one_jpeg*est_num_jpg_frames,actual_num_frames_written_to_buf);
                decode_called = 1;
                String msg = "actual num frames decoded: "+ actual_num_frames_written_to_buf[0];
                System.out.println(msg);
                System.out.println("chunkid: "+ randint);
                
               int end_ind;
               int length_in_bytes;
               int start_ind = 0; 

               
               FileSystem fs =  FileSystem.get(new JobConf());

                //write to jpg files in hdfs
               //for(int i = 0;i < actual_num_frames_written_to_buf[0];i++)
               for(int i = 0; i < 50; i++) 
               {
          	   end_ind = jpg_indexvec[i]-1;
                   length_in_bytes = end_ind - start_ind+1;
                   byte [] temp_buf = new byte[end_ind-start_ind+1];
                   System.arraycopy(jpg_outputbuffer_java,start_ind,temp_buf,0,length_in_bytes);
                   start_ind = end_ind+1;
                   
                   
                   Path outFile = new Path("frame"+randint+"_"+i+".jpg");
                   FSDataOutputStream out = fs.create(outFile);
                   out.write(temp_buf,0,length_in_bytes);
                   out.close();  
               }
          
          }
      }
      else
      { 
          if(curpos + fileDatalen < temp_byte_array_len)
          {
               System.arraycopy(fileData,0,temp_byte_array,curpos,fileDatalen);
               curpos = curpos + fileDatalen;
               temp_byte_array[curpos] =0x0A;//new line char; 
               curpos = curpos+1;
           
          }
          else
          {
              int rem_bytes = temp_byte_array_len-curpos;
              System.arraycopy(fileData,0,temp_byte_array,curpos,rem_bytes);
              curpos = curpos + rem_bytes;
              temp_byte_array_filled = 1;
              System.err.println("byte array filled");
          }
      }
      System.err.println("curpos : " + curpos);
   
      // String msg = "actual num frames decoded: "+ actual_num_frames_written_to_buf[0];
       //System.err.println(msg);       
//log.info(msg);
	// feed value into JNI

            //taking one line at a time and tokenizing the same
          String line = value.toString();
          StringTokenizer tokenizer = new StringTokenizer(line);
         
          //iterating through all the words available in that line and forming the key value pair
            while (tokenizer.hasMoreTokens())
            {
               word.set(tokenizer.nextToken());
               //sending to output collector which inturn passes the same to reducer
                 output.collect(word, one);
                //context.write(word,one);
            }
	
       }
}

